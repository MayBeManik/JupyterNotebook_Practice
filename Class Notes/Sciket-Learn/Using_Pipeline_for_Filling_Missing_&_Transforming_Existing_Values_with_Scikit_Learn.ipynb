{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTyDN_BOb0Al"
      },
      "source": [
        "# Using Scikit-Learn's `Pipeline()` class for filling missing data and encoding categorical data\n",
        "\n",
        "This notebook extends the code in the \"Putting It All Together\" with some more explanations for what's going on behind the scenes.\n",
        "\n",
        "More specifically, what happens with the `Pipeline()` class as we use it to impute missing values and encode (turn into numbers) categorical values.\n",
        "\n",
        "The main takeaways:\n",
        "- Encode and fill your **categorical data** into numbers (on the whole dataset), if necessary.\n",
        "- Split your data (into train/test), always keep your training & test data separate.\n",
        "- Fill/transform the **numerical data** on the training set and test sets separately.\n",
        "- Using [`Pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) helps to ensure all of this is done for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PRVurQBcb_x"
      },
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqVN3A_ddVzR"
      },
      "source": [
        "**Note:** Since this notebook is running on Colab, the data has been imported directly from GitHub. It is the same data used in the videos.\n",
        "\n",
        "Data on GitHub: https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/data/car-sales-extended-missing-data.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxanCzCOdMIc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "c7125499-0089-4a8b-950c-79bde1821487"
      },
      "source": [
        "car_sales_missing = pd.read_csv(\"https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/data/car-sales-extended-missing-data.csv\")\n",
        "car_sales_missing.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Make</th>\n",
              "      <th>Colour</th>\n",
              "      <th>Odometer (KM)</th>\n",
              "      <th>Doors</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Honda</td>\n",
              "      <td>White</td>\n",
              "      <td>35431.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>15323.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BMW</td>\n",
              "      <td>Blue</td>\n",
              "      <td>192714.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>19943.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Honda</td>\n",
              "      <td>White</td>\n",
              "      <td>84714.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>28343.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Toyota</td>\n",
              "      <td>White</td>\n",
              "      <td>154365.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>13434.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nissan</td>\n",
              "      <td>Blue</td>\n",
              "      <td>181577.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14043.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Make Colour  Odometer (KM)  Doors    Price\n",
              "0   Honda  White        35431.0    4.0  15323.0\n",
              "1     BMW   Blue       192714.0    5.0  19943.0\n",
              "2   Honda  White        84714.0    4.0  28343.0\n",
              "3  Toyota  White       154365.0    4.0  13434.0\n",
              "4  Nissan   Blue       181577.0    3.0  14043.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwEtIuzRdwio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ad6acf58-d95e-4b21-c0ad-0802ad203a68"
      },
      "source": [
        "# Check missing values\n",
        "car_sales_missing.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Make             49\n",
              "Colour           50\n",
              "Odometer (KM)    50\n",
              "Doors            50\n",
              "Price            50\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbBfGLYzdztd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "63009b8d-e58c-4008-fabf-d95c23f96254"
      },
      "source": [
        "# Drop the rows with no labels\n",
        "car_sales_missing.dropna(subset=[\"Price\"], inplace=True)\n",
        "car_sales_missing.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Make             47\n",
              "Colour           46\n",
              "Odometer (KM)    48\n",
              "Doors            47\n",
              "Price             0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeUEEzJ4Cd8I"
      },
      "source": [
        "# Split data into X & y\n",
        "X = car_sales_missing.drop(\"Price\", axis=1)\n",
        "y = car_sales_missing[\"Price\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JVcgU6hePGd"
      },
      "source": [
        "Now we've dropped the rows with no labels and split our data into `X` and `y`, let's create a [`Pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) (or a few of them) to fill the rest of the missing values, encode them if necessary (turn them into numbers) and fit a model to them.\n",
        "\n",
        "A `Pipeline()` in Scikit-Learn is a class which allows us to put multiple steps, such as filling data and then modelling it, together sequentially.\n",
        "\n",
        "More specifically, we'll go through the following steps:\n",
        "1. Define categorical, door and numeric features.\n",
        "2. Build transformer `Pipeline()`'s for imputing missing data and encoding data.\n",
        "3. Combine our transformer `Pipeline()`'s with [`ColumnTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html).\n",
        "4. Build a `Pipeline()` to preprocess and model our data with the `ColumnTransformer()` and [`RandomForestRegressor()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n",
        "5. Split the data into train and test using `train_test_split()`.\n",
        "6. Fit the preprocessing and modelling `Pipeline()` on the training data.\n",
        "7. Score the preprocessing and modelling `Pipeline()` on the test data.\n",
        "\n",
        "\n",
        "Let's start with steps 1. and 2.\n",
        "\n",
        "We'll build the following:\n",
        "* A categorical transformer to fill our categorical values with the value `'missing'` and then one encode them.\n",
        "* A door transformer to fill the door column missing values with the value `4`.\n",
        "* A numeric transformer to fill the numeric column missing values with the mean of the rest of the column.\n",
        "\n",
        "All of these will be done with the `Pipeline()` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvWWN0OxeAhc"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer # this will help us fill missing values\n",
        "from sklearn.preprocessing import OneHotEncoder # this will help us turn our categorical variables into numbers\n",
        "\n",
        "# Define categorical columns\n",
        "categorical_features = [\"Make\", \"Colour\"]\n",
        "# Create categorical transformer (imputes missing values, then one hot encodes them)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "  ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "  ('onehot', OneHotEncoder(handle_unknown='ignore'))                                         \n",
        "])\n",
        "\n",
        "# Define door feature\n",
        "door_feature = [\"Doors\"]\n",
        "# Create door transformer (fills all door missing values with 4)\n",
        "door_transformer = Pipeline(steps=[\n",
        "  ('imputer', SimpleImputer(strategy='constant', fill_value=4)),\n",
        "])\n",
        "\n",
        "# Define numeric features\n",
        "numeric_features = [\"Odometer (KM)\"]\n",
        "# Create a transformer for filling all missing numeric values with the mean\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "  ('imputer', SimpleImputer(strategy='mean'))  \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-nfvdTteYSJ"
      },
      "source": [
        "Wonderful! Now we've got a way to fill our missing variables and turn them into numbers (one hot encode the categorical variables), let's take care of step 3.\n",
        "\n",
        "3. Combine our transformer `Pipeline()`'s with `ColumnTransformer()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY_Xet3gfcbm"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Create a column transformer which combines all of the other transformers \n",
        "# into one step\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "      # (name, transformer_to_use, features_to_use transform)\n",
        "      ('categorical', categorical_transformer, categorical_features),\n",
        "      ('door', door_transformer, door_feature),\n",
        "      ('numerical', numeric_transformer, numeric_features)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiDj2c4wfrBK"
      },
      "source": [
        "Okay, we've got a `ColumnTransformer()` saved to `preprocessor`, let's make another `Pipeline()` to combine it with a `RandomForestRegressor()` model and take care of step 4.\n",
        "\n",
        "4. Build a `Pipeline()` to preprocess and model our data with the `ColumnTransformer()` and`RandomForestRegressor()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmYFW7oSgAix"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create the preprocessing and modelling pipeline\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor), # this will fill our missing data and make sure it's all numbers\n",
        "                        ('regressor', RandomForestRegressor())]) # this will model our data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1tzTQkHgCp7"
      },
      "source": [
        "Creating the `model` pipeline like we did is like saying to Scikit-Learn, \"Hey, preprocess all of the data we pass you first (using `preprocessor`) and then model it with `RandomForestRegressor()`.\n",
        "\n",
        "Let's now do step 5 and 6.\n",
        "\n",
        "5. Split the data into train and test using `train_test_split()`.\n",
        "6. Fit the preprocessing and modelling `Pipeline()` on the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zps_jcu0g3I9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "cdf168ab-9495-4a4a-af7c-5783eb74387a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and teset sets\n",
        "np.random.seed(42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Fit the model on the training data \n",
        "# (note: when fit() is called with a Pipeline(), fit_transform() is used for transformers)\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('categorical',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('imputer',\n",
              "                                                                   SimpleImputer(add_indicator=False,\n",
              "                                                                                 copy=True,\n",
              "                                                                                 fill_value='missing',\n",
              "                                                                                 missing_values=nan,\n",
              "                                                                                 strategy='constant',\n",
              "                                                                                 verbose=0)),\n",
              "                                                                  ('onehot',\n",
              "                                                                   OneHotEncoder(categ...\n",
              "                 RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
              "                                       criterion='mse', max_depth=None,\n",
              "                                       max_features='auto', max_leaf_nodes=None,\n",
              "                                       max_samples=None,\n",
              "                                       min_impurity_decrease=0.0,\n",
              "                                       min_impurity_split=None,\n",
              "                                       min_samples_leaf=1, min_samples_split=2,\n",
              "                                       min_weight_fraction_leaf=0.0,\n",
              "                                       n_estimators=100, n_jobs=None,\n",
              "                                       oob_score=False, random_state=None,\n",
              "                                       verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg8jzOs8g9Oe"
      },
      "source": [
        "If it all worked, we should've seen a print out of the steps in the modelling `Pipeline()`.\n",
        "\n",
        "We've got 1 step left and that's to evaluate our model pipeline on the test data.\n",
        "\n",
        "7. Score the preprocessing and modelling `Pipeline()` on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqaLoJBMhVD4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2786b34-1df4-4974-ae77-c487eefef317"
      },
      "source": [
        "# Score the model on the data \n",
        "# (note: when score() or  predict() is called with a Pipeline(), transform() is used for transformers)\n",
        "model.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22188417408787875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbUs-hlshGjl"
      },
      "source": [
        "Nice! Our `Pipeline()` steps seem to have worked for the test dataset as well.\n",
        "\n",
        "There are a few things going on behind the scenes here. The main ones being how the `Pipeline()` class deals with data transformation.\n",
        "\n",
        "## Pipeline behind the scenes\n",
        "\n",
        "When filling **numerical data**, it's important **not** to use values from the test set to fill values in the training set. Since we're trying to predict on the test set, this would be like taking information from the future to fill in the past.\n",
        "\n",
        "Let's have an example.\n",
        "\n",
        "In our case, the `Odometer (KM)` column is missing values. We could fill every value in the column (before splitting it into train and test) with the `mean()`. But this would result in using information from the test set to fill the training set (because we fill the whole column before the split).\n",
        "\n",
        "Instead, we split the data into train and test sets first (still with missing values). Then calculate the `mean()` of the `Odometer (KM)` column on the training set and use it to fill the **training set** missing values *as well as* the **test set** missing values. \n",
        "\n",
        "Now you might be asking, how does this happen?\n",
        "\n",
        "Well, behind the scenes, `Pipeline()` calls a couple of methods:\n",
        "1. `fit_transform()` - in our case, this computes the `mean()` of the `Odometer (KM)` column and then transforms the rest of the column on the **training data**. It also stores the `mean()` in memory.\n",
        "2. `transform()` - uses the saved `mean()` of the `Odometer (KM)` column and transforms the **test** values.\n",
        "\n",
        "Wait, wait, wait. This is confusing... how does the `Pipeline()` know what the training and test data are? We never told it?\n",
        "\n",
        "You're right.\n",
        "\n",
        "The magic trick is:\n",
        "* `fit_transform()` is only ever used when calling `fit()` on your `Pipeline()` (in our case, when we used `model.fit(X_train, y_train)`.\n",
        "* `transform()` is only ever used when calling `score()` or `predict()` on your `Pipeline()` (in our case, `model.score(X_test, y_test)`.\n",
        "\n",
        "![what's happening with Pipeline](https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/images/sklearn-whats-happening-with-pipeline.png)\n",
        "\n",
        "This means, when our missing **numerical values** get calculated and filled (using `fit_transform()`), they only happen on the training data (as long as you only pass `X_train` and `y_train` to `model.fit()`).\n",
        "\n",
        "And since they get saved in memory, when we call `model.score(X_test, y_test)` and subsequently `transform()`, the test data gets preprocessed with information from the training set (using the past to try and predict the future, not the other way round).\n",
        "\n",
        "### What about categorical values?\n",
        "\n",
        "Since they usually don't depend on each other, categorical values are okay to be filled across sets and examples.\n",
        "\n",
        "Okay, knowing all this, let's cross-validate our model pipeline using [cross_val_score()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKXSaKvihpC6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f8b35659-568e-43e0-b68e-1009095dd50d"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Cross-validate our pipeline model\n",
        "cross_val_score(model, X, y).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22152985494597335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MgQH2HFiE3Q"
      },
      "source": [
        "Since our `model` is an instance of `Pipeline()`, the same steps as we discussed above happen here with the `cross_val_score()`.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "If you'd like to dig deeper into what's happening here, I'd suggest the following reading resources and next steps.\n",
        "* **Reading:** [Scikit-Learn Pipeline() documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
        "* **Reading:** [Imputing missing values before building an estimator](https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html) (compares different methods of imputing values).\n",
        "* **Practice:** Try [tuning model hyperparameters with a `Pipeline()` and `GridSearchCV()`](https://scikit-learn.org/stable/modules/grid_search.html#composite-estimators-and-parameter-spaces)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY3ALu6uC4ma"
      },
      "source": [
        "How about all in 1 cell?\n",
        "\n",
        "(the code below runs the same as the code above, just in 1 cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMlgP9K91ZOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d50b46f-b5a5-4765-b78d-932bd852b24b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Import data from GitHub\n",
        "car_sales_missing = pd.read_csv(\"https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/data/car-sales-extended-missing-data.csv\")\n",
        "\n",
        "# Drop the rows with no labels\n",
        "car_sales_missing.dropna(subset=[\"Price\"], inplace=True)\n",
        "\n",
        "# Split data into X (data) & y (labels)\n",
        "X = car_sales_missing.drop(\"Price\", axis=1)\n",
        "y = car_sales_missing[\"Price\"]\n",
        "\n",
        "# Define categorical columns\n",
        "categorical_features = [\"Make\", \"Colour\"]\n",
        "# Create categorical transformer (imputes missing values, then one hot encodes them)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "  ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "  ('onehot', OneHotEncoder(handle_unknown='ignore'))                                         \n",
        "])\n",
        "\n",
        "# Define door feature\n",
        "door_feature = [\"Doors\"]\n",
        "# Create door transformer (fills all door missing values with 4)\n",
        "door_transformer = Pipeline(steps=[\n",
        "  ('imputer', SimpleImputer(strategy='constant', fill_value=4)),\n",
        "])\n",
        "\n",
        "# Define numeric featrue\n",
        "numeric_features = [\"Odometer (KM)\"]\n",
        "# Create a transformer for filling all missing numeric values with the mean\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "  ('imputer', SimpleImputer(strategy='mean'))  \n",
        "])\n",
        "\n",
        "# Create a column transformer which combines all of the other transformers \n",
        "# into one step\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "      ('categorical', categorical_transformer, categorical_features),\n",
        "      ('door', door_transformer, door_feature),\n",
        "      ('numerical', numeric_transformer, numeric_features)\n",
        "])\n",
        "\n",
        "# Create the model pipeline\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor), # this will fill our missing data and make sure it's all numbers\n",
        "                        ('regressor', RandomForestRegressor())]) # this will model our data\n",
        "\n",
        "# Split data into train and teset sets\n",
        "np.random.seed(42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Fit the model on the training data \n",
        "#(note: when fit() is called with a Pipeline(), fit_transform() is used for transformers)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Score the model on the data \n",
        "# (note: when score() or  predict() is called with a Pipeline(), transform() is used for transformers)\n",
        "model.score(X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22188417408787875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}